{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axh_KnxwbOtC"
      },
      "source": [
        "# **Retrieval-Augmented Generation (RAG) Pipeline Components**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpypNTWSbRyK"
      },
      "source": [
        "\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "RAG pipelines combine document retrieval with generation, enabling language models to provide grounded, context-aware answers. This section breaks down each component in the pipeline, showing their roles, how they work, and example code using LangChain.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTumyLAXbVdK"
      },
      "source": [
        "## **1. Document Loader**\n",
        "\n",
        "**Purpose:**  \n",
        "Load various document formats (TXT, PDF, DOCX, HTML, etc.) into LangChain `Document` objects for processing.\n",
        "\n",
        "**Function:**  \n",
        "Transforms raw files into structured text chunks that the pipeline can work with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHoE2QN9b4wi"
      },
      "source": [
        "**Other Loaders:**\n",
        "\n",
        "* `PyPDFLoader` — loads PDF files  \n",
        "* `UnstructuredLoader` — handles HTML, DOCX, and other complex formats\n",
        "\n",
        "These loaders parse documents and extract text for further processing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjJ3y0xCzaD5",
        "outputId": "cb4d0c58-5193-4acd-add6-9ef96d399393"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Create an instance for the specified file\n",
        "loader = TextLoader(\"Company_sample.txt\")\n",
        "\n",
        "# load the content fof the file\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "9acKqNxIzlqE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il7XjoSK0OEt",
        "outputId": "d0483bd7-ec17-47b6-a9ff-d6a826a05271"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACME Corporation - Company Overview and Operations Manual\n",
            "====================================================================\n",
            "Version 2.1 - Last Updated: January 2024\n",
            "\n",
            "1. COMPANY MISSION AND VISION\n",
            "   Our mission is to deliver innovative technology solutions that empower\n",
            "   businesses to achieve their full potential. We envision a future where\n",
            "   technology seamlessly integrates with human creativity to solve complex\n",
            "   challenges and drive sustainable growth.\n",
            "\n",
            "2. COMPANY HISTORY\n",
            "   ACME Corporation was founded in 2010 by a team of passionate engineers\n",
            "   and business leaders. Starting with just five employees in a small office,\n",
            "   we have grown to become a leading technology solutions provider with\n",
            "   over 500 employees across 12 countries. Our journey has been marked by\n",
            "   continuous innovation, strategic partnerships, and an unwavering commitment\n",
            "   to customer success.\n",
            "\n",
            "3. CORE VALUES\n",
            "   Innovation: We embrace new ideas and technologies that push boundaries.\n",
            "   Integrity: We conduct business with honesty, transparency, and ethical\n",
            "   standards in all our interactions.\n",
            "   Excellence: We strive for the highest quality in everything we do.\n",
            "   Collaboration: We believe in the power of teamwork and mutual respect.\n",
            "   Sustainability: We are committed to environmental responsibility and\n",
            "   social impact.\n",
            "\n",
            "4. ORGANIZATIONAL STRUCTURE\n",
            "   ACME Corporation operates with a matrix organizational structure that\n",
            "   combines functional expertise with project-based teams. Our leadership\n",
            "   team includes the CEO, CFO, CTO, COO, and VP of Sales, each leading\n",
            "   specialized departments while collaborating on strategic initiatives.\n",
            "   The company is divided into four major divisions: Engineering, Sales\n",
            "   and Marketing, Operations, and Human Resources.\n",
            "\n",
            "5. PRODUCTS AND SERVICES\n",
            "   We offer a comprehensive suite of enterprise software solutions including\n",
            "   cloud-based CRM platforms, data analytics tools, custom software development,\n",
            "   and IT consulting services. Our flagship product, ACME Enterprise Suite,\n",
            "   serves over 2,000 clients worldwide. Additionally, we provide specialized\n",
            "   services in cybersecurity, digital transformation, and AI implementation.\n",
            "\n",
            "6. TARGET MARKETS\n",
            "   Our primary market segments include mid to large-sized enterprises in the\n",
            "   financial services, healthcare, manufacturing, and retail sectors. We\n",
            "   also serve government agencies and non-profit organizations. Our solutions\n",
            "   are designed to scale from 50 to 50,000+ employees, making us versatile\n",
            "   partners for businesses at various stages of growth.\n",
            "\n",
            "7. COMPETITIVE ADVANTAGES\n",
            "   What sets ACME Corporation apart is our unique combination of technical\n",
            "   excellence and deep industry expertise. Our proprietary algorithms and\n",
            "   machine learning models provide insights that competitors cannot match.\n",
            "   Additionally, our 24/7 customer support and dedicated account management\n",
            "   teams ensure our clients receive personalized attention and rapid problem\n",
            "   resolution.\n",
            "\n",
            "8. CLIENT SUCCESS STORIES\n",
            "   One of our most notable achievements was helping Global Finance Corp\n",
            "   reduce operational costs by 35% through our automated workflow solutions.\n",
            "   Another success story involves MedTech Solutions, which improved patient\n",
            "   care outcomes by implementing our data analytics platform. These results\n",
            "   demonstrate our commitment to delivering measurable value to every client.\n",
            "\n",
            "9. RESEARCH AND DEVELOPMENT\n",
            "   We invest 15% of our annual revenue in research and development activities.\n",
            "   Our R&D team consists of 80 engineers and data scientists working on\n",
            "   cutting-edge technologies including artificial intelligence, blockchain,\n",
            "   quantum computing applications, and sustainable technology solutions.\n",
            "   Our innovation lab has filed 47 patents in the past three years.\n",
            "\n",
            "10. QUALITY ASSURANCE\n",
            "    Quality is embedded in every aspect of our operations. We follow ISO\n",
            "    9001:2015 quality management standards and have achieved SOC 2 Type II\n",
            "    certification. Our software development processes adhere to agile\n",
            "    methodologies with continuous integration and automated testing at every\n",
            "    stage. We maintain a customer satisfaction score of 4.7 out of 5.0.\n",
            "\n",
            "11. DATA SECURITY AND COMPLIANCE\n",
            "    Data security is paramount at ACME Corporation. We are GDPR compliant,\n",
            "    HIPAA certified for healthcare clients, and maintain PCI DSS Level 1\n",
            "    certification for payment processing. Our security team conducts regular\n",
            "    penetration testing and vulnerability assessments. All data is encrypted\n",
            "    both in transit and at rest using industry-standard protocols.\n",
            "\n",
            "12. SUSTAINABILITY INITIATIVES\n",
            "    We are committed to carbon neutrality by 2030 and have implemented\n",
            "    comprehensive sustainability programs. Our offices use 100% renewable\n",
            "    energy, and we have reduced paper consumption by 90% through digital\n",
            "    transformation initiatives. We also support reforestation projects and\n",
            "    have partnered with environmental organizations to offset our carbon\n",
            "    footprint.\n",
            "\n",
            "13. EMPLOYEE BENEFITS AND CULTURE\n",
            "    Our employees enjoy competitive compensation packages including health\n",
            "    insurance, dental and vision coverage, retirement plans with company\n",
            "    matching, and generous paid time off. We offer flexible work arrangements,\n",
            "    professional development opportunities, and employee wellness programs.\n",
            "    Our culture emphasizes work-life balance, diversity, and inclusion.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPcdKGk_cBzM"
      },
      "source": [
        "\n",
        "\n",
        "## **2. Text Splitter**\n",
        "\n",
        "**Purpose:**  \n",
        "Split large documents into smaller, manageable chunks.\n",
        "\n",
        "**Function:**  \n",
        "Smaller chunks improve embedding quality and retrieval accuracy.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XUXnSZavcGPa"
      },
      "outputs": [],
      "source": [
        "# Step # 1 = Library Import\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Step # 2 = Create a Splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "# chunk_size = number of letters\n",
        "# chunk_overlap = each piece shares 50 letters with the next one\n",
        "\n",
        "# step 3 : Cut the document into chunks\n",
        "chunks = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evGH8mcJcW5j"
      },
      "source": [
        "**Why overlap?**  \n",
        "Overlap (e.g., 50 characters) preserves context between chunks, helping the retriever maintain continuity in answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onXtaJnnctjC"
      },
      "source": [
        "## **3. Embeddings**\n",
        "\n",
        "**Purpose:**  \n",
        "Convert text chunks into numerical vectors that capture semantic meaning.\n",
        "\n",
        "**Function:**  \n",
        "Transforms words and sentences into dense vectors so that similar texts have nearby vectors.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W7x5RM1Ecz2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1760c2c8-58a8-4158-9787-cce669d28cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/475.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.8/475.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain-google-genai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HTULJZB5cwyT"
      },
      "outputs": [],
      "source": [
        "# Step-1 import the library\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model= \"models/gemini-embedding-001\",\n",
        "    google_api_key = \"AIzaSyAaRCWlyIQyR5YdahHyKhjFpRNNqX2wMmY\"\n",
        ")\n",
        "\n",
        "# We an convert the words into numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLSgMHCgdDJa"
      },
      "source": [
        "**Concept:**\n",
        "\n",
        "* Related terms like “artificial intelligence” and “machine learning” have close vectors  \n",
        "* Unrelated terms like “pizza” and “equation” have distant vectors\n",
        "\n",
        "Embeddings allow semantic search beyond exact keyword matching.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr2H1YLSdIo0"
      },
      "source": [
        "## **4. Vector Store (FAISS)**\n",
        "\n",
        "**Purpose:**  \n",
        "Store embeddings and enable fast similarity search.\n",
        "\n",
        "**Function:**  \n",
        "Index vector embeddings so queries can quickly retrieve the nearest (most relevant) documents.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gy8R0aiAebqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7281010a-3627-4c63-db4d-4db2dca844e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3zeAIsGXdRaL"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Step 1 = Create Chunks\n",
        "# Step 2 = Create embeddings like [0.1,0.2,0.9,12.1]\n",
        "# Step 3 = Store the embeddings into vectors store\n",
        "# Step 4 = Remember each chunk, and its embedding\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks,embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNCPRxBpeeUx"
      },
      "source": [
        "**Features of FAISS:**\n",
        "\n",
        "* Efficient k-nearest neighbor search  \n",
        "* Fast, in-memory vector similarity retrieval  \n",
        "* Scalable to millions of documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4KClFyeh0j"
      },
      "source": [
        "## 5. Retriever\n",
        "\n",
        "**Purpose:**  \n",
        "Fetch the top relevant document chunks from the vector store based on a user query.\n",
        "\n",
        "**Function:**  \n",
        "Given a query, it returns the most semantically relevant text pieces.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vwkQY4-qekt6"
      },
      "outputs": [],
      "source": [
        "# Without retriever\n",
        "# You: I need books about dogs\n",
        "# Library : I have 10,000 books, take them\n",
        "# You: Too much information\n",
        "\n",
        "# Retirever\n",
        "# You: I need books about dogs\n",
        "# Library : here are the best 3 books about dogs\n",
        "# You: Perfect\n",
        "\n",
        "# Ask a question: \"What is a cat\"\n",
        "# Goes into vector store\n",
        "# Retrieves the best 3 relevant chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type = \"similarity\",\n",
        "    k = 3\n",
        ")"
      ],
      "metadata": {
        "id": "HLkCJw7v_mAL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lqhqPqemZ6"
      },
      "source": [
        "**Retrieval methods:**\n",
        "\n",
        "* **Similarity Search:** Returns top-k closest vectors.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmgBVQlFes5i"
      },
      "source": [
        "\n",
        "## 6. Prompt Template\n",
        "\n",
        "**Purpose:**  \n",
        "Control how retrieved documents and user query are presented to the language model.\n",
        "\n",
        "**Function:**  \n",
        "Formats the prompt to optimize generation quality and context usage.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ePDkKGolewQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac4ccd0-b999-4b69-a738-5639c6a6d374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant that answers questions based on the provided context.\n",
            "\n",
            "    Context:\n",
            "    ACME corporation was founded in 2010.\n",
            "\n",
            "    Question: When was the company founded\n",
            "\n",
            "    Answer: Provide a clear and concise answer based on the context above, if the context doesn't contain enough information to answer the answer then say so\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Prompt Template = Fill in the blank worksheet for AI\n",
        "# We are making a recpipe that tells AI: \"Here is the information, here is the user question, now answer it\"\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    # There are two empty spaces in this worksheet\n",
        "    # context = The information from our documents\n",
        "    # question = what is the user question\n",
        "    input_variables = {\"context\", \"question\"},\n",
        "\n",
        "    template = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer: Provide a clear and concise answer based on the context above, if the context doesn't contain enough information to answer the answer then say so\"\"\"\n",
        "\n",
        ")\n",
        "\n",
        "# Test our template\n",
        "# fill the Worksheet with information\n",
        "test_prompt = prompt_template.format(\n",
        "    context=\"ACME corporation was founded in 2010.\",\n",
        "    question = \"When was the company founded\"\n",
        ")\n",
        "\n",
        "# Print the answer\n",
        "print(test_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACewF7K4eyma"
      },
      "source": [
        "Custom prompts help guide the LLM’s reasoning or style.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrpcTp9De1Ra"
      },
      "source": [
        "## **7. LLM (Generator) – GeminiAI**\n",
        "\n",
        "**Purpose:**  \n",
        "Generate final answers, summaries, or responses based on the retrieved context.\n",
        "\n",
        "**Function:**  \n",
        "Consumes the combined prompt (retrieved text + query) and outputs human-like text.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yk3jCNeYe7Bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391a9cc8-f1bd-45a7-efb0-6d8dbcf6a1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# We have to connect our pipeline to an LLM\n",
        "# It will read the context and the user question and will respond\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    google_api_key = \"AIzaSyBgEamZfyVXET4jo6UlFroPr8Rn6z3cA6s\",\n",
        "    temperature = 0.5,\n",
        "    convert_system_message_to_human = True # Talk to it like a person\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What is the capital of france?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCOT450fMdS"
      },
      "source": [
        "Parameters like `temperature` control creativity vs. precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnML6pTwfOn6"
      },
      "source": [
        "\n",
        "## 8. RAG Chain (RetrievalQA)\n",
        "\n",
        "**Purpose:**  \n",
        "Combine retriever and generator into an end-to-end pipeline.\n",
        "\n",
        "**Function:**  \n",
        "Automatically retrieve relevant documents and generate a grounded answer in one step.\n",
        "\n",
        "**Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RORFSoSrfQdM"
      },
      "outputs": [],
      "source": [
        "# Step # 1 = Look in the vector store to find the best possible answers/context\n",
        "# Step # 2 = Read the answers\n",
        "# Step # 3 = Tells you the answer\n",
        "\n",
        "def rag(query):\n",
        "  # Step 1: Look in the vector store\n",
        "  docs = retriever.invoke(query)\n",
        "\n",
        "  # Step 2: Combine all of these chunks, text\n",
        "  context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "  # Step # 3: Fill in the prompt template\n",
        "  prompt = prompt_template.format(context=context, question=query)\n",
        "\n",
        "  # Step 4: Ask the LLM to combine the responses and give an asnwer\n",
        "  response = llm.invoke(prompt)\n",
        "  answer = response.content\n",
        "\n",
        "  return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"what is the name of company\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fP2qASMVo8Xm",
        "outputId": "196cd153-7dfc-46c9-ad59-131d8503260d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The name of the company is ACME Corporation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"what are the products provided by the company\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xyqVbFVZ81yA",
        "outputId": "38fd886f-f254-45f8-f2ec-b88f61a895f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The company provides ACME Enterprise Suite, cloud-based CRM platforms, and data analytics tools. They also offer custom software development and IT consulting services.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"what do you know about abis working in ACME company\")"
      ],
      "metadata": {
        "id": "5Hdsz0SS8-Kg",
        "outputId": "cc367852-8876-446e-8b99-b72ca9634526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I\\'m sorry, but the provided context does not contain any information about \"abis\" working in ACME company.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8C54T0fRTy"
      },
      "source": [
        "This abstracts the workflow into a single callable chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Bw6PyJhu5Z"
      },
      "source": [
        "# **LangChain RAG Pipeline Architecture**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou4iq1GIh0J6"
      },
      "source": [
        "---\n",
        "\n",
        "##  What is RAG?\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)** is an architecture that enhances LLM responses by retrieving relevant documents from a knowledge base before generating an answer. This allows the model to stay factual, up-to-date, and context-aware without retraining.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9AzDfmJh4Oh"
      },
      "source": [
        "\n",
        "\n",
        "##  Visual Flow\n",
        "\n",
        "```plaintext\n",
        "                +-------------------+\n",
        "User Query -->  |  LangChain RAG    |\n",
        "                +---------+---------+\n",
        "                          |\n",
        "           +--------------+--------------+\n",
        "           |                             |\n",
        "  +--------v---------+        +----------v----------+\n",
        "  |  Retriever       |        |      Gemini LLM     |\n",
        "  | (FAISS Vector DB)|        |   (Text Generator)  |\n",
        "  +------------------+        +---------------------+\n",
        "           |                             |\n",
        "  +--------v-----------------------------v---------+\n",
        "  |       Final Answer / Summary from Gemini       |\n",
        "  +------------------------------------------------+\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3zaPMqh7Di"
      },
      "source": [
        "##  Component Breakdown\n",
        "\n",
        "| **Component**       | **Role**                                                                 |\n",
        "|---------------------|--------------------------------------------------------------------------|\n",
        "| **User Query**      | Natural language question from the user                                 |\n",
        "| **LangChain RAG**   | Orchestrates the flow: gets query → retrieves docs → gets final answer  |\n",
        "| **Retriever (FAISS)** | Searches stored documents based on semantic similarity                 |\n",
        "| **Gemini LLM**      | Uses context + query to generate a relevant, concise response           |\n",
        "| **Final Answer**    | The generated output shown to the user                                  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL3ZSL_0iLkJ"
      },
      "source": [
        "##  Example Scenario\n",
        "\n",
        "> **User Query:** \"What are the symptoms of diabetes?\"\n",
        "\n",
        "1. **LangChain RAG** receives the query.  \n",
        "2. **Retriever (FAISS)** finds the top 3 chunks from a medical textbook on diabetes.  \n",
        "3. **Gemini LLM** takes the query and the chunks and generates:\n",
        "\n",
        "> *\"Common symptoms of diabetes include increased thirst, frequent urination, fatigue, and blurred vision.\"*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1N78ohYiNqB"
      },
      "source": [
        "\n",
        "##  Why Use RAG with LangChain?\n",
        "\n",
        "-  Combines **search + generation**\n",
        "-  Keeps answers **grounded in source documents**\n",
        "-  Handles **custom domain knowledge** (e.g., legal, medical, academic)\n",
        "-  **Gemini** adds fluent natural language generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpIL2By0prWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}